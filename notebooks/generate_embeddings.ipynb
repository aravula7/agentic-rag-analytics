{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955dac44",
   "metadata": {},
   "source": [
    "1. Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3a2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve repo paths\n",
    "from pathlib import Path\n",
    "\n",
    "CWD = Path.cwd()\n",
    "REPO_ROOT = CWD.parent if CWD.name.lower() == \"notebooks\" else CWD\n",
    "\n",
    "ENV_PATH = REPO_ROOT / \".env\"\n",
    "EMBEDDINGS_DIR = REPO_ROOT / \"embeddings\"\n",
    "SCHEMA_DOWNLOAD_DIR = REPO_ROOT / \"schema_download\"\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\".env:\", ENV_PATH)\n",
    "print(\"embeddings/:\", EMBEDDINGS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11097789",
   "metadata": {},
   "source": [
    "2. Load OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ede8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded and OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(dotenv_path=ENV_PATH, override=True)\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key or len(api_key.strip()) < 20:\n",
    "    raise ValueError(\n",
    "        f\"OPENAI_API_KEY not found in: {ENV_PATH}\\n\"\n",
    "        \"Add this line to your .env:\\n\"\n",
    "        \"OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"OPENAI_API_KEY loaded and OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd94cc5",
   "metadata": {},
   "source": [
    "3. Fetch Schema Markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "637c1aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local schema files:\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\customers.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\products.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\orders.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\order_items.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\subscriptions.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\churn_predictions.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\forecast_predictions.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\relationships.md\n",
      " - c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\schema\\source\\analytics_patterns.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SCHEMA_DIR = REPO_ROOT / \"schema\" / \"source\"\n",
    "if not SCHEMA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Schema directory not found: {SCHEMA_DIR}\")\n",
    "\n",
    "SCHEMA_FILES = [\n",
    "    \"customers.md\",\n",
    "    \"products.md\",\n",
    "    \"orders.md\",\n",
    "    \"order_items.md\",\n",
    "    \"subscriptions.md\",\n",
    "    \"churn_predictions.md\",\n",
    "    \"forecast_predictions.md\",\n",
    "    \"relationships.md\",\n",
    "    \"analytics_patterns.md\",\n",
    "]\n",
    "\n",
    "missing = [f for f in SCHEMA_FILES if not (SCHEMA_DIR / f).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing schema files in {SCHEMA_DIR}: {missing}\")\n",
    "\n",
    "fetched = {fn: (SCHEMA_DIR / fn).read_text(encoding=\"utf-8\") for fn in SCHEMA_FILES}\n",
    "\n",
    "print(\"Loaded local schema files:\")\n",
    "for fn in SCHEMA_FILES:\n",
    "    print(\" -\", SCHEMA_DIR / fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f098893",
   "metadata": {},
   "source": [
    "4. Chunk schema markdown into sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "020d83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking complete\n",
      "   • Files processed: 9\n",
      "   • Total chunks:    81\n",
      "   • Chunks per file:\n",
      "     - customers.md: 9\n",
      "     - products.md: 9\n",
      "     - orders.md: 9\n",
      "     - order_items.md: 10\n",
      "     - subscriptions.md: 9\n",
      "     - churn_predictions.md: 9\n",
      "     - forecast_predictions.md: 9\n",
      "     - relationships.md: 7\n",
      "     - analytics_patterns.md: 10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_whitespace(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_by_h2(markdown_text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split markdown into chunks based on '## ' headers.\n",
    "    If a file has no '##' headers, returns a single FULL_DOCUMENT chunk.\n",
    "    \"\"\"\n",
    "    text = normalize_whitespace(markdown_text)\n",
    "\n",
    "    if re.search(r\"(?m)^\\s*##\\s+\", text) is None:\n",
    "        return [{\"section\": \"FULL_DOCUMENT\", \"content\": text}]\n",
    "\n",
    "    parts = re.split(r\"(?m)^\\s*(##\\s+.+?)\\s*$\", text)\n",
    "    chunks: List[Dict[str, str]] = []\n",
    "\n",
    "    # Optional preamble (content before first ##)\n",
    "    preamble = parts[0].strip()\n",
    "    if preamble:\n",
    "        chunks.append({\"section\": \"PREAMBLE\", \"content\": preamble})\n",
    "\n",
    "    # Header/body pairs\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i].strip()\n",
    "        body = parts[i + 1].strip() if i + 1 < len(parts) else \"\"\n",
    "        section_name = re.sub(r\"^\\s*##\\s+\", \"\", header).strip()\n",
    "        chunk_text = normalize_whitespace(f\"{header}\\n\\n{body}\")\n",
    "        if chunk_text:\n",
    "            chunks.append({\"section\": section_name, \"content\": chunk_text})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Build chunk records\n",
    "all_chunks: List[Dict] = []\n",
    "for filename, md in fetched.items():\n",
    "    table = filename.replace(\".md\", \"\")\n",
    "    chunks = split_by_h2(md)\n",
    "\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"table\": table,\n",
    "            \"filename\": filename,\n",
    "            \"section\": ch[\"section\"],\n",
    "            \"chunk_index\": idx,\n",
    "            \"text\": ch[\"content\"],\n",
    "        })\n",
    "\n",
    "# Professional summary output (no noisy previews)\n",
    "total_chunks = len(all_chunks)\n",
    "by_file = Counter(c[\"filename\"] for c in all_chunks)\n",
    "\n",
    "print(\"Chunking complete\")\n",
    "print(f\"   • Files processed: {len(fetched)}\")\n",
    "print(f\"   • Total chunks:    {total_chunks}\")\n",
    "print(\"   • Chunks per file:\")\n",
    "for fn in SCHEMA_FILES:\n",
    "    print(f\"     - {fn}: {by_file.get(fn, 0)}\")\n",
    "\n",
    "# Guardrails for your expected chunk range (no hard fail, just a warning)\n",
    "if total_chunks < 40 or total_chunks > 120:\n",
    "    print(\"Note: Chunk count is outside the expected ~50–80 range. \"\n",
    "          \"This is not necessarily wrong, but you may want to verify headings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8d84b",
   "metadata": {},
   "source": [
    "5. Initialize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7caa8304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB initialized\n",
      "   • Persist directory: c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\embeddings\n",
      "   • Collection name:   agentic_rag_analytics_schema\n",
      "   • Total chunks to embed: 81\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from datetime import datetime\n",
    "\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=str(EMBEDDINGS_DIR))\n",
    "\n",
    "COLLECTION_NAME = \"agentic_rag_analytics_schema\"\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\n",
    "        \"project\": \"agentic-rag-analytics\",\n",
    "        \"purpose\": \"schema_embeddings\",\n",
    "        \"embedding_model\": \"text-embedding-3-small\",\n",
    "        \"created_at_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"schema_source\": \"local_files\",\n",
    "        \"schema_path\": \"schema/source/*.md\",\n",
    "    },\n",
    "    embedding_function=None  # IMPORTANT: we provide embeddings manually\n",
    ")\n",
    "\n",
    "print(\"ChromaDB initialized\")\n",
    "print(\"   • Persist directory:\", EMBEDDINGS_DIR)\n",
    "print(\"   • Collection name:  \", COLLECTION_NAME)\n",
    "print(\"   • Total chunks to embed:\", total_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfaa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing collection deleted: agentic_rag_analytics_schema\n",
      "Fresh collection ready: agentic_rag_analytics_schema\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chroma_client.delete_collection(COLLECTION_NAME)\n",
    "    print(\"Existing collection deleted:\", COLLECTION_NAME)\n",
    "except Exception:\n",
    "    # If it doesn't exist yet, that's fine\n",
    "    print(\"No existing collection to delete (first run)\")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\n",
    "        \"project\": \"agentic-rag-analytics\",\n",
    "        \"purpose\": \"schema_embeddings\",\n",
    "        \"embedding_model\": \"text-embedding-3-small\",\n",
    "        \"created_at_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"schema_source\": \"local_files\",\n",
    "        \"schema_path\": \"schema/source/*.md\",\n",
    "    },\n",
    "    embedding_function=None\n",
    ")\n",
    "\n",
    "print(\"Fresh collection ready:\", COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0cb8c",
   "metadata": {},
   "source": [
    "6. Generate Embeddings (batched) and store in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3841436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding run started\n",
      "   • Model:       text-embedding-3-small\n",
      "   • Chunks:      81\n",
      "   • Batch size:  64\n",
      "   • Batches:     2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches:   0%|          | 0/2 [00:00<?, ?it/s]Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
      "Embedding batches: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding run complete\n",
      "   • Stored vectors: 81\n",
      "   • Elapsed time:   4.40s\n",
      "   • Persist dir:    c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "BATCH_SIZE = 64  # ~81 chunks => usually 2 API calls\n",
    "\n",
    "def stable_id(item: dict) -> str:\n",
    "    \"\"\"\n",
    "    Stable deterministic ID (reruns won't duplicate if collection isn't wiped).\n",
    "    \"\"\"\n",
    "    content_hash = hashlib.md5(item[\"text\"].encode(\"utf-8\")).hexdigest()\n",
    "    key = f\"{item['filename']}|{item['section']}|{item['chunk_index']}|{content_hash}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Prepare records\n",
    "ids = [stable_id(c) for c in all_chunks]\n",
    "documents = [c[\"text\"] for c in all_chunks]\n",
    "metadatas = [{\n",
    "    \"table\": c[\"table\"],\n",
    "    \"filename\": c[\"filename\"],\n",
    "    \"section\": c[\"section\"],\n",
    "    \"chunk_index\": c[\"chunk_index\"],\n",
    "    \"source\": \"local_file\",\n",
    "    \"path\": f\"schema/source/{c['filename']}\",\n",
    "} for c in all_chunks]\n",
    "\n",
    "t0 = time.time()\n",
    "num_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(\"Embedding run started\")\n",
    "print(f\"   • Model:       {EMBED_MODEL}\")\n",
    "print(f\"   • Chunks:      {len(documents)}\")\n",
    "print(f\"   • Batch size:  {BATCH_SIZE}\")\n",
    "print(f\"   • Batches:     {num_batches}\")\n",
    "\n",
    "# Embed + store\n",
    "for start in tqdm(range(0, len(documents), BATCH_SIZE), desc=\"Embedding batches\"):\n",
    "    end = min(start + BATCH_SIZE, len(documents))\n",
    "\n",
    "    resp = client.embeddings.create(\n",
    "        model=EMBED_MODEL,\n",
    "        input=documents[start:end]\n",
    "    )\n",
    "    embeddings = [x.embedding for x in resp.data]\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids[start:end],\n",
    "        documents=documents[start:end],\n",
    "        metadatas=metadatas[start:end],\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(\"Embedding run complete\")\n",
    "print(f\"   • Stored vectors: {collection.count()}\")\n",
    "print(f\"   • Elapsed time:   {elapsed:.2f}s\")\n",
    "print(f\"   • Persist dir:    {EMBEDDINGS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96307aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output ready\n",
      "   • Manifest: c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\embeddings\\manifest.json\n",
      "   • Folder:   c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\embeddings\n",
      "   • Zip:      c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\embeddings.zip\n",
      "\n",
      "Tip: Right-click embeddings.zip in VS Code Explorer → 'Reveal in File Explorer' → share/download.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) Write a small manifest for traceability\n",
    "manifest = {\n",
    "    \"project\": \"agentic-rag-analytics\",\n",
    "    \"collection\": COLLECTION_NAME,\n",
    "    \"embedding_model\": EMBED_MODEL,\n",
    "    \"chunk_count\": len(all_chunks),\n",
    "    \"files\": SCHEMA_FILES,\n",
    "    \"schema_source\": \"local_files\",\n",
    "    \"schema_dir\": str(REPO_ROOT / \"schema\" / \"source\"),\n",
    "    \"persist_dir\": str(EMBEDDINGS_DIR),\n",
    "    \"created_at_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "}\n",
    "\n",
    "(EMBEDDINGS_DIR / \"manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# 2) Zip the embeddings folder\n",
    "zip_path = REPO_ROOT / \"embeddings.zip\"\n",
    "if zip_path.exists():\n",
    "    zip_path.unlink()\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in EMBEDDINGS_DIR.rglob(\"*\"):\n",
    "        z.write(p, p.relative_to(REPO_ROOT))\n",
    "\n",
    "print(\"Output ready\")\n",
    "print(\"   • Manifest:\", EMBEDDINGS_DIR / \"manifest.json\")\n",
    "print(\"   • Folder:  \", EMBEDDINGS_DIR)\n",
    "print(\"   • Zip:     \", zip_path)\n",
    "print(\"\\nTip: Right-click embeddings.zip in VS Code Explorer → 'Reveal in File Explorer' → share/download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87662f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully!\n",
      "Folder: c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\embeddings\n",
      "Zip: c:\\Users\\ravul\\Desktop\\ML Portfolio\\agentic-rag-analytics\\embeddings.zip\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings generated successfully!\")\n",
    "print(\"Folder:\", EMBEDDINGS_DIR)\n",
    "print(\"Zip:\", REPO_ROOT / \"embeddings.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
